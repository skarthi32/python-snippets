# https://github.com/bhave-sh/Hive_AWS-Snippets/blob/master/src/main/scripts/partitionExtractionUsingHiveCLI/runScript.sh
# https://github.com/rewindio/aws-athena-partition-autoloader/blob/main/src/app.py


def add_partition(session, database, table_name, partition, bucket):
    current_key = 0
    status = False

    sql = 'ALTER TABLE ' + table_name + ' ADD PARTITION ('

    partition_key_vals = partition.split('/')
    # Filter out any prefix dirs from the key
    partition_key_vals = [p for p in partition_key_vals if "=" in p]
    partiton_key_count = len(partition_key_vals)

    for part in partition_key_vals:
        current_key += 1
        key,val = part.split('=')

        sql += key + " = '" + val + "'"

        if current_key != partiton_key_count:
            sql += ", "
        else:
            sql += ") "

    sql += "LOCATION 's3://" + bucket + "/" + partition + "';"

#
# Write the list of table partitions to a cache file
#
def write_partition_cache(partitions, filename):
    with open(filename, 'w') as outfile:
        json.dump(partitions, outfile)

#
# Load the list of partitions from the cache file
#
def load_partition_cache(filename):
    data = dict()

    with open(filename) as json_file:
        data = json.load(json_file)

    return data

# https://github.com/gautam-shanbhag/AWS-Load-New-Partitions-Programmatically/blob/89c9ef9bccaa1917386bb9e5c025e25bf309af6c/loadPartitions.py#L88

import boto3
import re
import time
import botocore
import sys

s3Client = boto3.client('s3', region_name=params['region'])
s3Resource = boto3.resource('s3')

def s3CheckIfBucketExists(s3Resource, bucketName):
    try:
        s3Resource.meta.client.head_bucket(Bucket=bucketName)
        print("Athena Bucket exists")
        print("----------------------------------")
        print()
    except botocore.exceptions.ClientError as e:
        print("Athena Bucket does not exist.")
        print(e)
        print("----------------------------------")
        location = {'LocationConstraint': params['region']}
        s3Client.create_bucket(Bucket=params['s3Bucket'], CreateBucketConfiguration=location)
        print()
        print("Athena Bucket Created Successfully.")
        print()

def s3ListObject(s3, prefix):
    resultList = []
    result = s3.list_objects_v2(
        Bucket=params['s3Bucket'],
        Delimiter='/',
        Prefix=prefix
    )
    if result['KeyCount'] == 0:
        return False
    try:
        resultList.extend(result.get('CommonPrefixes'))
        while (result['IsTruncated']):
            result = s3.list_objects_v2(
                Bucket=params['s3Bucket'],
                Delimiter='/',
                Prefix=prefix,
                ContinuationToken=result['NextContinuationToken']
            )
            resultList.extend(result.get('CommonPrefixes'))
    except Exception as e:
        print("#~ FAILURE ~#")
        print("Error with :")
        print(result)
        raise

    return resultList

def cleanup(s3Resource, params):
    print('Cleaning Temp Folder Created: ')
    print(params['athenaResultBucket']+'/'+params["athenaResultFolder"]+'/')
    print()
    s3Resource.Bucket(params['athenaResultBucket']).objects.filter(Prefix=params["athenaResultFolder"]).delete()
    print('Cleaning Completed')
    print("----------------------------------")
    print()
    # s3Resource.Bucket(params['athenaResultBucket']).delete()

def split(l, n):
    # For item i in a range that is a length of l,
    for i in range(0, len(l), n):
        # Create an index range for l of n items:
        yield l[i:i+n]

# Parse S3 folder structure and create partition list
prefix = params['s3Folder']
yearFolders = s3ListObject(s3Client, prefix)
if yearFolders:
    monthList = []
    for year in yearFolders:
        result = s3Client.list_objects_v2(
            Bucket=params['s3Bucket'],
            Delimiter='/',
            Prefix=year.get('Prefix')
        )
        try:
            monthList.extend(result.get('CommonPrefixes'))
        except Exception as e:
            print("#~ FAILURE ~#")
            print("Error with :")
            print(result)
            raise

    s3List = []
    for thingType in monthList:
        string = thingType.get('Prefix').replace(params['s3Folder'], "")
        s3List.append(string.rstrip('/'))

    # To filter out default spark null partitions and folders like  _SUCCESS, _temporary, __HIVE_DEFAULT_PARTITION__
    s3List = [i for i in s3List if (('month' in i) and (i.startswith('year')) and not ('__HIVE_DEFAULT_PARTITION__' in i))]

    print("S3 Folder Structure At :")
    print(params['s3Bucket'] + '/' + params['s3Folder'])
    print("----------------------------------")
    print()
    print("S3 Partition List : ")
    print(s3List)
    print("----------------------------------")
    print()


    # Compare Athena Partition List with S3 Partition List
    resultSet = set(s3List) - set(athenaList)
    print("Result Set : ")
    print(resultSet)
    print("----------------------------------")
    print()


    # Create Alter Query for Athena
    try:
        if len(resultSet) != 0:
            print("Partition Count : " + str(len(resultSet)))
            result = split(list(resultSet), 1000)
            for resultSet in result:
                queryString = "ALTER TABLE " + params['tableName'] + " ADD IF NOT EXISTS PARTITION(" + repr(resultSet) + ")"
                queryString = queryString.replace("[", "")
                queryString = queryString.replace("]", "")
                queryString = queryString.replace("{", "")
                queryString = queryString.replace("}", "")
                queryString = queryString.replace(",", ") PARTITION(")
                queryString = queryString.replace("'", "")
                queryString = queryString.replace("date=", "date='")
                queryString = queryString.replace("/", "', ")
                print("Alter Query String : ")
                print(queryString)
                print("----------------------------------")
                print()

                # Run Alter Partition Query
                execution = athena_query(athenaClient, queryString)
                if execution['ResponseMetadata']['HTTPStatusCode'] == 200:
                    # Temp Folder Cleanup
                    cleanup(s3Resource, params)
                    print("*~ SUCCESS ~*")
                    print()
                else:
                    print("#~ FAILURE ~#")
                    print()

        else:
            # Temp Folder Cleanup
            cleanup(s3Resource, params)
            print()
            print("*~ SUCCESS ~*")

    except Exception as e:
            # Temp Folder Cleanup
            cleanup(s3Resource, params)
            print("#~ FAILURE ~#")
            print("Error with :")
            print(resultSet)
            print(e)
            raise


======================================================================================================

def add_new_partitions(table_source_location, database_name, target_table):
    """
    Compare existing prefixes in S3 against existing partitions in athena, then add any missing.
    Currently only supports one level of partition depth.
    :param table_source_location: location of data in S3
    :param database_name: athena database
    :param target_table: table to compare against
    """
    s3_hook = S3Hook()
    bucket, prefix = S3Hook.parse_s3_url(table_source_location)
    prefix = prefix.strip('/') + '/'
    partition_prefixes = s3_hook.list_prefixes(bucket, prefix, '/')
    partitions_pending = [p.strip('/').split('/')[-1] for p in partition_prefixes]

    # get list of current partitions for the table
    athena_hook = AthenaHook()
    partition_results = athena_hook.execute_query_with_results(
        query='SHOW PARTITIONS {table}'.format(table=target_table),
        database_name=database_name
    )

    # result looks like this [{'partition': 'created_date=2019-01-17'}, {'partition': 'created_date=2019-01-18'}]
    existing_partition_values = [partition_result['partition'] for partition_result in partition_results]

    partition_diffs = set(partitions_pending) - set(existing_partition_values)

    # create all new partitions
    partitions_to_add = []
    for partition_diff in partition_diffs:
        col_name, col_value = partition_diff.split('=')
        partitions_to_add.append("\tPARTITION ({col_name} = '{col_value}') LOCATION '{location}'".format(
            col_name=col_name,
            col_value=col_value,
            location='{}/{}'.format(table_source_location, partition_diff))
        )

    if partitions_to_add:
        add_partitions_query = 'ALTER TABLE {table} ADD\n{partitions}'.format(
            table=target_table,
            partitions='\n'.join(partitions_to_add)
        )

        print(add_partitions_query)

        athena_hook.execute_query(
            query=add_partitions_query,
            database_name=database_name
        )
